@article{Brock2018,
abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple "truncation trick," allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.},
archivePrefix = {arXiv},
arxivId = {1809.11096},
author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
eprint = {1809.11096},
file = {:Users/michaelgentnermac/OneDrive/Dokumente/02{\_}Masterstudium/01{\_}TU-M{\"{u}}nchen/02{\_}Semester 2/ADL4CV/Project/Literatur/1809.11096-2.pdf:pdf},
pages = {1--35},
title = {{Large Scale GAN Training for High Fidelity Natural Image Synthesis}},
url = {http://arxiv.org/abs/1809.11096},
year = {2018}
}
@article{Vondrick2016,
abstract = {We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.},
archivePrefix = {arXiv},
arxivId = {1609.02612},
author = {Vondrick, Carl and Pirsiavash, Hamed and Torralba, Antonio},
eprint = {1609.02612},
file = {:Users/michaelgentnermac/OneDrive/Dokumente/02{\_}Masterstudium/01{\_}TU-M{\"{u}}nchen/02{\_}Semester 2/ADL4CV/Project/Literatur/paper.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {613--621},
title = {{Generating videos with scene dynamics}},
year = {2016}
}
@article{Gulrajani2017,
abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models with continuous generators. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
archivePrefix = {arXiv},
arxivId = {1704.00028},
author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
eprint = {1704.00028},
file = {:Users/michaelgentnermac/OneDrive/Dokumente/02{\_}Masterstudium/01{\_}TU-M{\"{u}}nchen/02{\_}Semester 2/ADL4CV/Project/Literatur/7159-improved-training-of-wasserstein-gans.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {5768--5778},
title = {{Improved training of wasserstein GANs}},
volume = {2017-Decem},
year = {2017}
}
@article{Karras2018,
abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 10242. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.},
archivePrefix = {arXiv},
arxivId = {1710.10196},
author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
eprint = {1710.10196},
file = {:Users/michaelgentnermac/OneDrive/Dokumente/02{\_}Masterstudium/01{\_}TU-M{\"{u}}nchen/02{\_}Semester 2/ADL4CV/Project/Literatur/1710.10196.pdf:pdf},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
pages = {1--26},
title = {{Progressive growing of GANs for improved quality, stability, and variation}},
year = {2018}
}
@article{Aigner2019,
abstract = {{\textless}p{\textgreater}{\textless}strong{\textgreater}Abstract.{\textless}/strong{\textgreater} We introduce a new {\textless}i{\textgreater}encoder-decoder GAN{\textless}/i{\textgreater} model, {\textless}i{\textgreater}FutureGAN{\textless}/i{\textgreater}, that predicts future frames of a video sequence conditioned on a sequence of past frames. During training, the networks solely receive the raw pixel values as an input, without relying on additional constraints or dataset specific conditions. To capture both the spatial and temporal components of a video sequence, spatio-temporal 3d convolutions are used in all encoder and decoder modules. Further, we utilize concepts of the existing {\textless}i{\textgreater}progressively growing GAN (PGGAN){\textless}/i{\textgreater} that achieves high-quality results on generating high-resolution single images. The FutureGAN model extends this concept to the complex task of video prediction. We conducted experiments on three different datasets, {\textless}i{\textgreater}MovingMNIST{\textless}/i{\textgreater}, {\textless}i{\textgreater}KTH Action{\textless}/i{\textgreater}, and {\textless}i{\textgreater}Cityscapes{\textless}/i{\textgreater}. Our results show that the model learned representations to transform the information of an input sequence into a plausible future sequence effectively for all three datasets. The main advantage of the FutureGAN framework is that it is applicable to various different datasets without additional changes, whilst achieving stable results that are competitive to the state-of-the-art in video prediction. The code to reproduce the results of this paper is publicly available at https://github.com/TUM-LMF/FutureGAN.{\textless}/p{\textgreater}},
archivePrefix = {arXiv},
arxivId = {1810.01325},
author = {Aigner, S. and K{\"{o}}rner, M.},
doi = {10.5194/isprs-archives-xlii-2-w16-3-2019},
eprint = {1810.01325},
file = {:Users/michaelgentnermac/OneDrive/Dokumente/02{\_}Masterstudium/01{\_}TU-M{\"{u}}nchen/02{\_}Semester 2/ADL4CV/Project/Literatur/FutureGan.pdf:pdf},
journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
pages = {3--11},
title = {{Futuregan: Anticipating the Future Frames of Video Sequences Using Spatio-Temporal 3D Convolutions in Progressively Growing Gans}},
volume = {XLII-2/W16},
year = {2019}
}
@article{Clark2019,
abstract = {Generative models of natural images have progressed towards high fidelity samples by the strong leveraging of scale. We attempt to carry this success to the field of video modeling by showing that large Generative Adversarial Networks trained on the complex Kinetics-600 dataset are able to produce video samples of substantially higher complexity and fidelity than previous work. Our proposed model, Dual Video Discriminator GAN (DVD-GAN), scales to longer and higher resolution videos by leveraging a computationally efficient decomposition of its discriminator. We evaluate on the related tasks of video synthesis and video prediction, and achieve new state-of-the-art Fr$\backslash$'echet Inception Distance for prediction for Kinetics-600, as well as state-of-the-art Inception Score for synthesis on the UCF-101 dataset, alongside establishing a strong baseline for synthesis on Kinetics-600.},
archivePrefix = {arXiv},
arxivId = {1907.06571},
author = {Clark, Aidan and Donahue, Jeff and Simonyan, Karen},
eprint = {1907.06571},
file = {:Users/michaelgentnermac/OneDrive/Dokumente/02{\_}Masterstudium/01{\_}TU-M{\"{u}}nchen/02{\_}Semester 2/ADL4CV/Project/Literatur/1907.06571.pdf:pdf},
pages = {1--21},
title = {{Adversarial Video Generation on Complex Datasets}},
url = {http://arxiv.org/abs/1907.06571},
year = {2019}
}
@article{Chen2019,
abstract = {Generative adversarial networks have achieved great success in unpaired image-to-image translation. Cycle consistency allows modeling the relationship between two distinct domains without paired data. In this paper, we propose an alternative framework, as an extension of latent space interpolation, to consider the intermediate region between two domains during translation. It is based on the fact that in a flat and smooth latent space, there exist many paths that connect two sample points. Properly selecting paths makes it possible to change only certain image attributes, which is useful for generating intermediate images between the two domains. We also show that this framework can be applied to multi-domain and multi-modal translation. Extensive experiments manifest its generality and applicability to various tasks.},
author = {Chen, Ying-Cong and Xu, Xiaogang and Tian, Zhuotao and Jia, Jiaya},
file = {:Users/michaelgentnermac/Documents/ADL4CV/adl4cvreport/literature/CHEN.pdf:pdf},
journal = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {2408--2416},
title = {{Homomorphic Latent Space Interpolation for Unpaired Image-to-image Translation}},
url = {http://jiaya.me/papers/unpairimagetranslation{\_}cvpr19.pdf},
year = {2019}
}
@article{Zhu2016,
abstract = {Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result. Unless the user has considerable artistic skill, it is easy to “fall off” the manifold of natural images while editing. In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network. We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times. The model automatically adjusts the output keeping all edits as realistic as possible. All our manipulations are expressed in terms of constrained optimization and are applied in near-real time. We evaluate our algorithm on the task of realistic photo manipulation of shape and color. The presented method can further be used for changing one image to look like the other, as well as generating novel imagery from scratch based on user's scribbles.},
archivePrefix = {arXiv},
arxivId = {1609.03552},
author = {Zhu, Jun Yan and Kr{\"{a}}henb{\"{u}}hl, Philipp and Shechtman, Eli and Efros, Alexei A.},
doi = {10.1007/978-3-319-46454-1_36},
eprint = {1609.03552},
file = {:Users/michaelgentnermac/Documents/ADL4CV/adl4cvreport/literature/iGAN.pdf:pdf},
isbn = {9783319464534},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {597--613},
title = {{Generative visual manipulation on the natural image manifold}},
volume = {9909 LNCS},
year = {2016}
}
@article{Chen2016,
abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound of the mutual information objective that can be optimized efficiently. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing supervised methods. For an up-to-date version of this paper, please see https://arxiv.org/abs/1606.03657.},
archivePrefix = {arXiv},
arxivId = {1606.03657},
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
eprint = {1606.03657},
file = {:Users/michaelgentnermac/Documents/ADL4CV/adl4cvreport/literature/infoGAN.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {2180--2188},
title = {{InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets}},
year = {2016}
}
@article{Arjovsky2017,
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
archivePrefix = {arXiv},
arxivId = {1701.07875},
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
eprint = {1701.07875},
file = {:Users/michaelgentnermac/Documents/ADL4CV/adl4cvreport/literature/WGAN.pdf:pdf},
title = {{Wasserstein GAN}},
url = {http://arxiv.org/abs/1701.07875},
year = {2017}
}
@inproceedings{DCGAN,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
booktitle = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
eprint = {1511.06434},
file = {:Users/michaelgentnermac/Documents/ADL4CV/adl4cvreport/literature/DCGAN.pdf:pdf},
pages = {1--16},
title = {{Unsupervised representation learning with deep convolutional generative adversarial networks}},
year = {2016}
}
@article{INST,
abstract = {It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github at https://github.com/DmitryUlyanov/texture{\_}nets. Full paper can be found at arXiv:1701.02096.},
archivePrefix = {arXiv},
arxivId = {1607.08022},
author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
eprint = {1607.08022},
file = {:Users/michaelgentnermac/Documents/ADL4CV/adl4cvreport/literature/Instance{\_}Norm.pdf:pdf},
number = {2016},
title = {{Instance Normalization: The Missing Ingredient for Fast Stylization}},
url = {http://arxiv.org/abs/1607.08022},
year = {2016}
}
@article{Karras2018a,
abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
archivePrefix = {arXiv},
arxivId = {1812.04948},
author = {Karras, Tero and Laine, Samuli and Aila, Timo},
eprint = {1812.04948},
file = {:Users/michaelgentnermac/Documents/ADL4CV/adl4cvreport/literature/NVIDIA{\_}faces.pdf:pdf},
title = {{A Style-Based Generator Architecture for Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1812.04948},
year = {2018}
}
